<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>http://dhhoang.github.io/</id>
	<title>Huy Hoang blog</title>
	<link rel="self" href="http://dhhoang.github.io/" />
	<rights>2020</rights>
	<updated>2020-09-28T16:04:47Z</updated>
	<subtitle>Huy Hoang's blog. Coding, Engineering, Science, and other stuffs</subtitle>
	<entry>
		<id>http://dhhoang.github.io/posts/state-machine-ds-vi</id>
		<title>[P3] Mô hình State Machine trong Distributed Systems</title>
		<link href="http://dhhoang.github.io/posts/state-machine-ds-vi" />
		<updated>2020-09-25T00:00:00Z</updated>
		<content>&lt;p&gt;Khi tìm hiểu và làm việc với Distributed Systems, chúng ta có thể sẽ hay bắt gặp những cụm từ như: &lt;em&gt;Replicated State machine&lt;/em&gt;, &lt;em&gt;Log replication&lt;/em&gt; hay &lt;em&gt;Event-driven&lt;/em&gt;, &lt;em&gt;Event-sourcing&lt;/em&gt;. Về bản chất, những khái niệm này đều được xây dựng xung quanh mô hình &amp;quot;Máy trạng thái&amp;quot; (State machine). Bài viết này sẽ mô tả mô hình này và lý do tại sao State machine lại được áp dụng rộng rãi trong Distributed Systems.&lt;/p&gt;
&lt;h2 id="nhac-lai-so-qua-mot-vai-khai-niem"&gt;Nhắc lại sơ qua một vài khái niệm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trạng thái (state) của một process&lt;/strong&gt; có thể hiểu là tập hợp các giá trị của các biến (variable) trong process đó. VD, nếu process &lt;code&gt;p1&lt;/code&gt; có thể có state &lt;code&gt;S1 = {x=1, y=2}&lt;/code&gt;. Thường trong thực tế, một process sẽ có cả dữ liệu trong RAM lẫn trên đĩa cứng, chúng ta coi tất cả các dữ liệu này là state của process đó.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State machine&lt;/strong&gt; trong tin học là một mô hình tính toán, trong đó một máy tính (machine) thay đổi trạng thái (state) dựa theo chuỗi input mà nó được cung cấp. Ta hãy lấy ví dụ một bài toán: xác định xem trong một dãy số nhị phân (VD: &lt;code&gt;110100101&lt;/code&gt;), số chữ số &lt;code&gt;0&lt;/code&gt; là chẵn hay lẻ. Bài toán này có thể giải bằng cấu trúc State machine, trong đó chuỗi input là chuỗi các chữ số nhị phân, và &amp;quot;state&amp;quot; có thể là &amp;quot;chẵn&amp;quot; (S1) hay &amp;quot;lẻ&amp;quot; (S2). State machine này bắt đầu từ trạng thái S1 (có 0 chữ số &lt;code&gt;0&lt;/code&gt;), và với mỗi giá trị chữ số, nó thay đổi trạng thái theo như hình minh họa bên dưới. Trạng thái cuối cùng cũng là kết quả (output) của bài toán.&lt;/li&gt;
&lt;li&gt;Về &lt;strong&gt;thứ tự của các phần tử trong một tật hợp&lt;/strong&gt;. Giả sử ta có một tập hợp &lt;code&gt;S={x1,x2,x3,x4}&lt;/code&gt;. Nếu tất cả các phần tử của tập hợp này có thể được sắp xếp theo một thứ tự cố định (VD: &lt;code&gt;x1 &amp;lt; x2 &amp;lt; x3 &amp;lt; x4&lt;/code&gt;), bao gồm cả tính bắc cầu (&lt;code&gt;x1 &amp;lt; x2, x2 &amp;lt; x3 =&amp;gt; x1 &amp;lt; x3&lt;/code&gt;) và phản đối xứng (&lt;code&gt;x1 &amp;lt;= x2 &amp;amp; x2 &amp;lt;= x1 =&amp;gt; x1 = x2&lt;/code&gt;), thì tập hợp này được gọi là có thứ tự &lt;strong&gt;toàn phần&lt;/strong&gt; (total order). Nếu không thỏa mãn điều kiện trên thì tập hợp này có thứ tự &lt;strong&gt;một phần&lt;/strong&gt; (partial order). Khái niệm này quan trọng trong việc xác định thứ tự các sự kiện trong Distributed Systems. Bạn có thể tham khảo kĩ hơn về lý thuyết thứ tự của tập hợp tại &lt;a href="http://book.mixu.net/distsys/time.html"&gt;đây&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://github.com/dhhoang/dhhoang.github.io/raw/gh-pages/sm_acceptor.png" class="img-fluid" alt="State machine" title="Logo Title Text 1" /&gt;&lt;/p&gt;
&lt;h2 id="bat-au-tu-mot-cau-truc-quen-thuoc-log"&gt;Bắt đầu từ một cấu trúc quen thuộc: &lt;em&gt;log&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Hầu như ai làm dev cũng đều phải quen làm việc với &lt;em&gt;log&lt;/em&gt;. Nói đơn giản thì log là một chuỗi các dữ kiện giúp lưu lại thông tin về những sự kiện đã xảy ra trong quá trình phần mềm vận hành. Mỗi dữ kiện được gọi là một &lt;em&gt;entry&lt;/em&gt;. Ví dụ, đây là một đoạn log của &lt;code&gt;dpkg&lt;/code&gt; trên Ubuntu:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ tail -f /var/log/dpkg.log

2020-09-21 01:51:20 status installed man-db:amd64 2.9.1-1
2020-09-21 01:51:21 trigproc dbus:amd64 1.12.16-2ubuntu2.1 &amp;lt;none&amp;gt;
2020-09-21 01:51:22 status half-configured dbus:amd64 1.12.16-2ubuntu2.1
2020-09-21 01:51:23 status installed dbus:amd64 1.12.16-2ubuntu2.1
2020-09-21 01:51:24 trigproc mime-support:all 3.64ubuntu1 &amp;lt;none&amp;gt;
2020-09-21 01:51:25 status half-configured mime-support:all 3.64ubuntu1
2020-09-21 01:51:26 status installed mime-support:all 3.64ubuntu1
2020-09-21 01:51:27 trigproc initramfs-tools:all 0.136ubuntu6.3 &amp;lt;none&amp;gt;
2020-09-21 01:51:28 status half-configured initramfs-tools:all 0.136ubuntu6.3
2020-09-21 01:51:29 status installed initramfs-tools:all 0.136ubuntu6.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Một log entry trong ví dụ trên bao gồm hai thông tin: một là mô tả về sự kiện, hai là thời điểm (timestamp) xảy ra của sự kiện. Loại log quen thuộc này thường được gọi là &lt;code&gt;Application log&lt;/code&gt;, mục đích chính của nó là giúp chúng ta truy tìm thông tin khi cần thiết (ví dụ như để truy vết bug chẳng hạn). Lưu ý là, các entry trong log luôn có thứ tự &lt;em&gt;toàn phần&lt;/em&gt; (total order). Đối với log kể trên thì giá trị của timestamp là yếu tố xác định thứ tự của các entry. Log entry chỉ có thể được thêm vào log, chứ không thể bị xóa đi hay thay đổi. Thuật ngữ mô tả quy tắc này là &amp;quot;append-only&amp;quot;.&lt;/p&gt;
&lt;p&gt;Trong lý thuyết về Distributed Systems, hai thành phần thông tin kể trên của log (nội dung và thứ tự của sự kiện) đóng vai trò quan trọng. Các hệ thống Distributed Systems hiện nay đều được xây dựng xung quanh việc đồng thuận về nội dung của chuỗi log. Hệ quả này dựa trên nguyên lý (tạm gọi là nguyên lý &lt;em&gt;nhân bản trạng thái&lt;/em&gt;) như sau:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nếu ta có hai process tất định (deterministic) như nhau và chúng xử lý cùng chuỗi dữ liệu đầu vào (input) như nhau, thì chúng sẽ có đầu ra (output) và trạng thái cuối cùng (state) như nhau.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Deterministic process&lt;/em&gt;  ở đây được hiểu là process chỉ vận hành dựa trên việc xử lý input chứ không phụ thuộc vào các yếu tố bên ngoài (VD như yếu tố thời gian, yếu tố ngẫu nhiên...). Hiển nhiên là hai hệ thống deterministic như nhau nếu cùng nhận một chuỗi input sẽ cho ra kết quả như nhau. Do đó, chúng ta có thể lưu trữ hoàn toàn lịch sử trạng thái của một hệ thống bằng việc lưu trữ chuỗi input dưới dạng log.&lt;/p&gt;
&lt;p&gt;Đến đây thì chúng ta có thể nhận thấy được sự liên quan giữa &amp;quot;log&amp;quot; và &amp;quot;State machine&amp;quot;. Nếu ta có một &lt;em&gt;deterministic process&lt;/em&gt; được cấu trúc theo dạng State machine và một log lưu lại các input của process này, ta có được toàn bộ thông tin về quá trình hoạt động của process. Lấy ví dụ, một hệ thống ngân hàng cần lưu trữ thông tin tài khoản của khách hàng. Ta có thể lưu những yêu cầu xử lý của khách hàng dưới dạng input log như sau:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Tài khoản X được khởi tạo.
2. Tài khoản X nạp 100.000 VND.
3. Tài khoản Y được khởi tạo.
4. Tài khoản X chuyển cho tài khoản Y 50.000 VND.
5. Tài khoản Y rút ra 20.000 VND.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Từ log trên ta có thể dễ dàng tính ra trạng thái cuối cùng của hệ thống này (sau sự kiện &lt;code&gt;5&lt;/code&gt;) là &lt;code&gt;{X=50.000, Y=30.000}&lt;/code&gt;. Tuy nhiên, ngoài ra, cấu trúc input log  này có một vài ưu điểm như sau:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thứ nhất, cấu trúc này lưu trữ được nhiều thông tin hơn là việc lưu trạng thái thông thường. Ở đây ta mặc nhiên có được một lịch sử giao dịch cho tài khoản X và Y. Điều này cho phép ta có thể xác định được trạng thái của hệ thống trong bất kì thời điểm nào.&lt;/li&gt;
&lt;li&gt;Thứ hai, cấu trúc này hỗ trợ khả năng đối phó sự cố (fault tolerant) của hệ thống. Nếu như một server bị crash trong quá trình hoạt động, thì khi phục hồi, ta có thể tái thiết lại trạng thái cho server đó bằng cách &amp;quot;chạy&amp;quot; lại input log này (đây còn được gọi là 'log replay').&lt;/li&gt;
&lt;li&gt;Kết cấu này cũng mang tính đàn hồi (scalable) cao. Dữ liệu log kể trên có thể được nhân bản hay chuyển hóa thành các dạng dữ liệu khác nhằm phục vụ cho các mục đích khác nhau. Ta sẽ bàn về điều này nhiều hơn trong phần &lt;strong&gt;Event sourcing&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="log-replication-va-replicated-state-machine"&gt;Log replication và Replicated State machine.&lt;/h2&gt;
&lt;p&gt;Ở &lt;a href="/posts/consensus-problem-vi"&gt;phần 2&lt;/a&gt;, chúng ta đã nói sơ qua về tầm quan trọng của bài toán đồng thuận (consensus) trong DS. Bài toán này thực ra là phiên bản đơn giản hóa, vì yêu cầu đặt ra chỉ là giúp các server đồng thuận 1 giá trị. Trên thực tế, các hệ thống khi vận hành phải tiếp nhận yêu cầu của người dùng và thay đổi trạng thái liên tục, do đó bài toán ở đây là các server phải đồng thuận một chuỗi giá trị thay đổi theo thời gian. Các server cũng có thể bị mất kết nối hoặc crash, khiến cho việc đồng bộ thông tin giữa các máy càng trở nên phức tạp. Lấy ví dụ, nếu một server crash và sau đó phục hồi, khi đó trạng thái của server này đã khác biệt so với các server khác. Làm thế nào để ta biết được trạng thái nào mới là trạng thái đúng?&lt;/p&gt;
&lt;p&gt;Mô hình &lt;em&gt;Replicated State machine&lt;/em&gt; (các máy trạng thái nhân bản), hay &lt;em&gt;Log replication&lt;/em&gt;, được phát triển nhằm giải quyết vấn đề này. Trong mô hình này, mỗi server trong hệ thống được cấu trúc dưới dạng State machine, tương đồng với &lt;em&gt;Deterministic process&lt;/em&gt; được nêu ở phần trên. Log trong trường hợp này là chuỗi các input mà hệ thống nhận được từ người dùng (client), sắp xếp theo thứ tự thời gian (total order). State machine và log này được nhân bản ra tất cả các server trong hệ thống nhằm giúp các máy đồng thuận với nhau. Có nhiều cơ chế để thực hiện thao tác nhân bản này, ví dụ như &lt;em&gt;leader/follower&lt;/em&gt; (thuật toán Raft), hay &lt;em&gt;atomic broadcast&lt;/em&gt; (thuật toán ZAB). Chúng ta sẽ tìm hiểu sâu hơn về các thuật toán này trong các bài sau.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/dhhoang/dhhoang.github.io/raw/gh-pages/replicated_state_machine.jpg" class="img-fluid" alt="Replicated State Machine" title="Logo Title Text 1" /&gt;&lt;/p&gt;
&lt;p&gt;Điểm ưu việt của cơ chế này là, trong trường hợp một server nào đó fail, server này có thể phục hồi trạng thái bằng cách replay lại log. Các server cũng có thể so sánh phiên bản log của mình và xác định được phiên bản nào là phiên bản được cập nhật mới nhất (dựa trên thứ tự các entry trong log). Ngoài ra, nếu một server mới được thêm vào hệ thống, server này cũng có thể đồng thuận với các server còn lại thông qua cơ chế tương tự.&lt;/p&gt;
&lt;h2 id="moi-lien-he-en-event-sourcing"&gt;Mối liên hệ đến Event-sourcing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Replicated State machine&lt;/em&gt; (RSM) và &lt;em&gt;Event-sourcing&lt;/em&gt; (ES) là hai khái niệm khác nhau. RSM chỉ về mô hình consensus trong Distributed Systems nói chung, trong khi đó ES nói về cách tổ chức và sắp đặt dữ liệu của một ứng dụng cụ thể. Tuy nhiên, hai khái niệm này dựa trên cùng một nguyên lý &lt;em&gt;nhân bản trạng thái&lt;/em&gt; nêu trên, nên ta hãy bàn sơ qua về ES ở đây một chút.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Event-sourcing&lt;/em&gt; hoạt động dựa trên việc lưu lại dữ liệu của một hệ thống dưới dạng một chuỗi các sự kiện (event) theo thứ tự thời gian trong một cấu trúc Event Log. Event log này là dữ liệu duy nhất mô tả trạng thái của hệ thống, tại thời điểm hiện tại cũng như trong quá khứ. Kiến trúc ES hoạt động khác biệt so với kiểu trên trúc phổ thông thêm-sửa-xóa (CRUD). Thử ví dụ về hệ thống ngân hàng ở phần trước: một hệ thống theo kiểu CRUD có thể lưu mỗi tài khoản ở một dòng trong bảng SQL, trong khi đó, với ES, ta lưu tất cả các sự kiện trong hệ thống trong một chuỗi event duy nhất (xem VD ở trên). Cũng giống như input log, Event log cho phép tái tạo lại dữ liệu của hệ thống ở bất kì thời điểm nào.&lt;/p&gt;
&lt;p&gt;Điểm cộng lớn nhất của cách bố trí dữ liệu kiểu này nằm ở sự đơn giản: một chuỗi sự kiện duy nhất chứa đầy đủ các thông tin của hệ thống, từ số tiền hiện tại đến lịch sử giao dịch của một tài khoản. Theo kiểu CRUD, ta sẽ phải có một bảng riêng biệt để lưu lịch sử các giao dịch của các tài khoản, và sử dụng JOIN khi cần truy cập dữ liệu ở nhiều bảng khác nhau. Với ES, ta chỉ có một chuỗi duy nhất (thường được gọi là &lt;em&gt;single source of truth&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Sự đơn giản này giúp các hệ thống ES có tính đàn hồi (scalability) cao. Với kiểu CRUD, thường thì ta sẽ phải thiết kế Database nhằm tối ưu cho cả việc đọc và ghi dữ liệu. Tuy nhiên không phải dữ liệu nào cũng có thể tối ưu được một cách tuyệt đối. Hầu hết các hệ thống đều phải hy sinh hiệu năng cho việc đọc (hoặc ghi) để tối ưu cho hoạt động còn lại. Hơn nữa, các ứng dụng trên thực tế đều phải thay đổi theo thời gian để liên tục đáp ứng yêu cầu mới, do đó, việc tối ưu database ngay từ đầu gần như là bất khả thi.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/dhhoang/dhhoang.github.io/raw/gh-pages/event_sourcing_projection.png" class="img-fluid" alt="Event sourcing projection" title="Logo Title Text 1" /&gt;&lt;/p&gt;
&lt;p&gt;Với ES thì dữ liệu được tổ chức theo kiểu hoàn toàn khác biệt. Tất cả tiến trình ghi (write) đều có hiệu năng cao, do chỉ phải thêm (append) event (do event không được phép sửa/xóa). Các event này sau đó được chuyển hóa thành các dạng dữ liệu được tối ưu cho việc đọc (có thể là trong một bảng SQL hoặc các database khác). Quy trình này được gọi là &lt;em&gt;projection&lt;/em&gt; (Xem hình minh họa ở trên), trong đó Event log tại từng thời điểm nhất định được chuyển hóa thành các snapshot nhằm tối ưu việc truy cập dữ liệu. Điều này giúp cho hệ thống không bị bó buộc bởi cấu trúc quan hệ - thực thể (entity-relationship) trong các kiểu dữ liệu truyền thống.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Khi tìm hiểu và làm việc với Distributed Systems, chúng ta có thể sẽ hay bắt gặp những cụm từ như: &lt;em&gt;Replicated State machine&lt;/em&gt;, &lt;em&gt;Log replication&lt;/em&gt; hay &lt;em&gt;Event-driven&lt;/em&gt;, &lt;em&gt;Event-sourcing&lt;/em&gt;. Về bản chất, những khái niệm này đều được xây dựng xung quanh mô hình "Máy trạng thái" (State machine). Bài viết này sẽ mô tả mô hình này và lý do tại sao State machine lại được áp dụng rộng rãi trong Distributed Systems.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://dhhoang.github.io/posts/consensus-problem-vi</id>
		<title>[P2] Bài toán đồng thuận trong Distributed Systems</title>
		<link href="http://dhhoang.github.io/posts/consensus-problem-vi" />
		<updated>2020-09-01T00:00:00Z</updated>
		<content>&lt;p&gt;Ví dụ về Distributed Systems được phát triển và sử dụng trong thực tế bao gồm hệ thống Database (SQL, NoSQL), hệ thống Cache (Redis, Memcached), hệ thống Message-Queue hay Publish/Subscribe (Kafka, RabbitMQ ...). Những hệ thống này tuy có những tính năng khác nhau, tuy nhiên phần cốt lõi của chúng đều phải giải quyết một vấn đề cơ bản, đó là bài toán &lt;strong&gt;đồng thuận&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Như đã đề cập ở &lt;a href="/posts/distributed-system-overview"&gt;phần 1&lt;/a&gt;, một trông những tính chất của Distributed System là khả năng hoạt động như một thực thể thống nhất đối với người sử dụng. Để đạt được mục đích này thì trong quá trình hoạt động, các process trong hệ thống cần có được sự nhất trí về trạng thái dữ liệu (state) cũng như các bước vận hành tiếp theo (action). Do các process có thể không nằm cùng trên một máy tính vật lý nên chúng buộc phải giao tiếp qua network để đạt được sự nhất trí này. Tuy nhiên, cả máy tính lẫn network đều có thể gặp phải các sự cố, khiến cho việc giao tiếp để đảm bảo sự đồng nhất giữa các process trở nên không đơn giản.&lt;/p&gt;
&lt;p&gt;Cách đạt được sự nhất trí giữa các process trong Distributed Systems, kể cả khi gặp sự cố, được gọi là bài toán đồng thuận (Distributed Consensus). Để hiểu rõ hơn bài toán này, trong bài viết này chúng ta sẽ bàn về hai ví dụ mang tính minh họa.&lt;/p&gt;
&lt;h2 id="bai-toan-hai-vi-tuong-two-generals-problem"&gt;Bài toán hai vị tướng (Two generals problem)&lt;/h2&gt;
&lt;p&gt;Bài toán được phát biểu như sau: Có 2 vị tướng chỉ huy 2 đạo quân đóng ở các địa điểm khác nhau và cùng đối đầu với một đội quân đối phương. Hai vị tướng này cần đi đến một quyết định: cùng tấn công đối phương, hoặc cùng rút lui. Nếu họ cùng tấn công đối phương thì sẽ giành thắng lợi, nếu họ cùng rút lui thì sẽ bảo toàn được lực lượng. Ngược lại nếu hai vị tướng ra quyết định trái ngược (1 tấn công, 1 rút lui) thì họ sẽ bị đối phương đánh bại. Hai vị tướng có thể dùng giao liên để gửi thư cho nhau, tuy nhiên mỗi giao liên đều có khả năng bị đối thử bắt giữ, dẫn đến việc lá thư sẽ không được vận chuyển. Vậy 2 vị tướng trên cần dùng giao thức nào để có thể đi đến hành động thống nhất?&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Trước khi tiếp tục bài viết, mình khuyến khích các bạn hãy thử tự tìm cách giải cho bài toán này. Về cơ bản, bài toán yêu cầu một trong hai vị tướng phải đề xuất ra hành động và gửi thư cho vị tướng còn lại để yêu cầu sự nhất trí. Tuy nhiên, làm cách nào để vị tướng đề xuất đó biết được rằng thông tin đã đến được tay vị tướng kia hay chưa?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Đối với những bạn có hiểu biết về mảng Computer Networking, có thể bạn sẽ nhận thấy bài toán này có nhiều điểm tương đồng với giao thức TCP/IP, là giao thức mạng được phát triển nhằm cho phép người gửi tin có thể đảm bảo rằng thông tin đã đến được phía người nhận. Chúng ta có thể áp dụng cách tiếp cận tương tự cho bài toán này, dựa trên giao thức DATA/ACK của TCP/IP. Một vị tướng (&lt;code&gt;G1&lt;/code&gt;) sẽ đề xuất phương án và gửi thông tin cho người còn lại (&lt;code&gt;G2&lt;/code&gt;). Khi nhận được thư, &lt;code&gt;G2&lt;/code&gt; sẽ thực hiện theo phương án được đề xuất và gửi thư xác nhận (ACK) cho &lt;code&gt;G1&lt;/code&gt;. Nếu &lt;code&gt;G1&lt;/code&gt; nhận được thư xác nhận, &lt;code&gt;G1&lt;/code&gt; cũng sẽ thực hiện phương án đó.&lt;/p&gt;
&lt;p&gt;Tuy nhiên, những mẩu thư liên lạc kể trên đều có thể bị mất dọc đường, theo như điều kiện của bài toán. Nếu thư &lt;code&gt;G1-&amp;gt;G2&lt;/code&gt; bị mất, &lt;code&gt;G2&lt;/code&gt; sẽ không nhận được đề xuất. Tệ hơn nữa, nếu thư ACK bị mất, &lt;code&gt;G2&lt;/code&gt; sẽ tấn công trong khi &lt;code&gt;G1&lt;/code&gt; vẫn chưa quyết định được hành động. Vậy chúng ta giải quyết trường hợp này ra sao? Câu trả lời ngắn gọn ở đây là, &lt;strong&gt;bài toán không có lời giải hoàn hảo&lt;/strong&gt;. Thật vậy, đối phương trên lý thuyết có khả năng chặn được tất cả liên lạc giữa 2 vị tướng, và trong trường hợp đó sẽ không có cách nào đạt được sự đồng thuận cả.&lt;/p&gt;
&lt;p&gt;Ý nghĩa chính của bài toán 2 vị tướng có nhằm minh họa một điều, đó là không có lời giải &amp;quot;hoàn hảo&amp;quot; cho bài toán đồng thuận. Tuy nhiên, chúng ta có thể đưa ra một phương án mang tính &amp;quot;thực tế&amp;quot; hơn, bằng việc giả thiết rằng đối phương không phải lúc nào cũng có thể bắt được tất cả các thư liên lạc được gửi đi giữa 2 vị tướng. Theo đó, ta có thể cho &lt;code&gt;G1&lt;/code&gt; gửi lại đề xuất nếu không nhận được ACK sau một thời gian nào đó, cho đến khi nhận được ACK từ &lt;code&gt;G2&lt;/code&gt; (tương tự như cơ chế &lt;em&gt;retransmission&lt;/em&gt; của TCP/IP).&lt;/p&gt;
&lt;h2 id="bai-toan-cac-vi-tuong-byzantine-byzantine-generals-problem"&gt;Bài toán các vị tướng Byzantine (Byzantine General's problem)&lt;/h2&gt;
&lt;p&gt;Viết tắt là BGP, bài toán này được phát biểu như sau: Có &lt;code&gt;n&lt;/code&gt; vị tướng đến từ Byzantine (tên một đế quốc thời trung cổ nằm tại vùng Thổ Nhĩ Kỳ và đông nam châu Âu ngày nay), mỗi người chỉ huy một đội quân, cùng bao vây một thành trì của đối phương. Họ cần đạt được sự nhất trí về phương án hành động: hoặc tấn công (&lt;code&gt;A&lt;/code&gt;), hoặc rút lui (&lt;code&gt;R&lt;/code&gt;). Các vị tướng có thể liên lạc với nhau bằng cách gửi thư qua giao liên một cách an toàn (thông tin liên lạc không bị mất hay đánh tráo). Tuy nhiên, một vài trong số &lt;code&gt;n&lt;/code&gt; vị tướng này là gián điệp của đối phương, và có khả năng gửi những mẩu tin bất kì, nhằm ngăn cản việc nhất trí của các đội quân còn lại. Yêu cầu bài toán đặt ra là: 1) tìm ra một giao thức để các vị tướng (không phải gián điệp) có thể nhất trí về hành động (&lt;code&gt;A&lt;/code&gt; hoặc &lt;code&gt;R&lt;/code&gt;), 2) tìm hiểu xem số lượng gián điệp ảnh hưởng đến giao thức trên như thế nào?&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bài toán này được nêu ra bởi nhà khoa học máy tính Leslie Lamport trong bài báo &amp;quot;&lt;a href="https://dl.acm.org/doi/10.1145/357172.357176"&gt;The Byzantine Generals Problem&lt;/a&gt;&amp;quot;, và tên gọi của kiểu sự cố &amp;quot;Byzantine&amp;quot; (xem lại phần 1) được đặt theo tên bài toán này. Có nhiều người hay nhầm lẫn giữa bài toán này và bài toán 2 vị tướng (2GP) được nêu ở phần trước. Hai bài toán có một vài điểm khác biệt. Thứ nhất, BGP có một số lượng đội quân bất kì (thay vì chỉ 2 như 2GP). Thứ nhì, cả 2 vị tướng trong 2GP đều &amp;quot;trung thành&amp;quot;, trong khi BGP có một hoặc nhiều vị tướng là gián điệp. Thứ ba, ở 2GP, việc liên lạc giữa 2 đạo quân có thể bị gián đoạn, trong khi ở BGP chúng ta coi như liên lạc giữa các vị tướng là hoàn hảo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Để đi đến lời giải, chúng ta hay thử bắt đầu lý luận từ một vài trường hợp đơn giản. Với &lt;code&gt;n=1&lt;/code&gt;, bài toán trở nên tối giản, không có sự đồng thuận nào ở đây cả. Với &lt;code&gt;n=2&lt;/code&gt;, ta có một lời giải tương đối đơn giản: một vị tướng sẽ đề xuất phương án &lt;code&gt;v&lt;/code&gt; và gửi thư cho vị tướng còn lại, và cả 2 sẽ cùng thực hiện &lt;code&gt;v&lt;/code&gt;. Do liên lạc giữa 2 vị tướng được đảm bảo, chắc chắn họ sẽ đạt được sự đồng thuận. Trong trường hợp một trong hai vị tướng là gián điệp, bất kì hành động nào của người còn lại đều hợp lệ.&lt;/p&gt;
&lt;p&gt;Bài toán trở nên phức tạp hơn khi &lt;code&gt;n=3&lt;/code&gt;. Trong tình huống đơn giản nhất với &lt;code&gt;m=0&lt;/code&gt; gián điệp, lời giải của chúng ta tương tự như trường hợp &lt;code&gt;n=2&lt;/code&gt;. Một vị tướng &lt;code&gt;G1&lt;/code&gt; sẽ đề xuất phương án hành động &lt;code&gt;v&lt;/code&gt; và gửi thông tin cho các vị tướng còn lại. Các vị tướng còn lại khi nhận được tin sẽ thực hiện theo &lt;code&gt;v&lt;/code&gt;. Vậy nếu &lt;code&gt;m=1&lt;/code&gt; thì sao? Trong trường hợp này, người để xuất (&lt;code&gt;G1&lt;/code&gt;) có thể là gián điệp và sẽ gửi những thông tin trái ngược nhau cho 2 vị tướng còn lại (&lt;code&gt;G2&lt;/code&gt; và &lt;code&gt;G3&lt;/code&gt;). Chúng ta có thể thử dùng liên lạc giữa &lt;code&gt;G2&lt;/code&gt; và &lt;code&gt;G3&lt;/code&gt; để xác nhận lại thông tin. Kể cả vậy, nếu &lt;code&gt;G1&lt;/code&gt; là gián điệp, &lt;code&gt;G2&lt;/code&gt; và &lt;code&gt;G3&lt;/code&gt; vẫn sẽ nhận được những thông tin không thống nhất về phương án hành động &lt;code&gt;v&lt;/code&gt;. Thật vậy, nếu &lt;code&gt;G1&lt;/code&gt; gửi &lt;code&gt;A&lt;/code&gt; cho &lt;code&gt;G2&lt;/code&gt; và &lt;code&gt;R&lt;/code&gt; cho &lt;code&gt;G3&lt;/code&gt;, &lt;code&gt;G2&lt;/code&gt; và &lt;code&gt;G3&lt;/code&gt; sẽ nhận nhìn thấy những thông tin như sau:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// G2, G3 là các vector chứa thông tin phương án nhận được từ G1. Phần tử [0] là thông tin nhận được từ G1, [1] là thông tin nhận được từ vị tướng còn lại. 
G2: [A R]
G3: [R A]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Điều tương tự xảy ra khi &lt;code&gt;G2&lt;/code&gt; hay &lt;code&gt;G3&lt;/code&gt; là gián điệp. Điều này dẫn đến việc &lt;code&gt;G2&lt;/code&gt; hoặc &lt;code&gt;G3&lt;/code&gt; không xác định được gián điệp là ai, và không thể thống nhất được phương án &lt;code&gt;v&lt;/code&gt;. Một cách tổng quát hơn, BGP không có lời giải cho trường hợp &lt;code&gt;n&lt;/code&gt; vị tướng và &lt;code&gt;m&lt;/code&gt; gián điệp nếu &lt;code&gt;3m &amp;gt;=n &lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Nói cách khác, BGP chỉ có thể giải được khi số gián điệp nhỏ hơn 1/3 số vị tướng (&lt;code&gt;3m &amp;lt; n&lt;/code&gt;). Thuật toán để giải bài toán này được gọi tên là &lt;strong&gt;Byzantine Fault Tolerance&lt;/strong&gt; (BFT). Chi tiết về thuật toán (và cách chứng minh) tương đối phức tạp nên mình sẽ không trình bày ở đây (khuyến khích các bạn đọc thuật toán này trong bài báo của Lamport). Tuy nhiên mình sẽ tóm tắt một vài ý chính:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ý tưởng&lt;/strong&gt;: Các vị tướng được chia thành 2 vai trò: 1 vị tướng đóng vai trò chỉ huy (&lt;code&gt;C&lt;/code&gt;), những người còn lại là các phó tướng (&lt;code&gt;L&lt;/code&gt;). Chỉ huy sẽ đề xuất phương án &lt;code&gt;v&lt;/code&gt; và các phó tướng sẽ làm theo. Để đạt được yêu cầu bài toán kể cả khi chỉ huy C là gián điệp, các phó tướng sẽ thông tin cho nhau biết xem phương án &lt;code&gt;v&lt;/code&gt; mà mình nhận được là gì. VD, phó tướng &lt;code&gt;L3&lt;/code&gt; sẽ gửi &lt;code&gt;v3&lt;/code&gt; (nhận được từ &lt;code&gt;C&lt;/code&gt;) cho các phó tướng còn lại. Sau khi quá trình này kết thúc, mỗi phó tướng &lt;code&gt;L&lt;/code&gt; sẽ có được một vector các giá trị mà các phó tướng nhận được từ chỉ huy: &lt;code&gt;L = [v1 v2 ... v_(n-1)]&lt;/code&gt;, và sẽ chọn giá trị nào chiếm đa số.&lt;/li&gt;
&lt;li&gt;Tuy nhiên, &lt;code&gt;L3&lt;/code&gt; nói trên có thể là gián điệp và có thể gửi các giá trị &lt;code&gt;v3&lt;/code&gt; khác nhau cho các phó tướng còn lại. Vậy trường hợp này giải quyết ra sao? Nếu &lt;code&gt;L3&lt;/code&gt; là gián điệp thì &lt;code&gt;v3&lt;/code&gt; thực tế vô nghĩa. Điều quan trọng duy nhất ở đây là các phó tướng (không phải gián điệp) &lt;em&gt;đồng thuận&lt;/em&gt; với nhau về &lt;code&gt;v3&lt;/code&gt;. Ta có thể đạt được sự đồng thuận này bằng cách giải một bài toán đồng thuận &amp;quot;con&amp;quot;, tương tự như bài toán gốc nhưng loại bỏ đi chỉ huy &lt;code&gt;C&lt;/code&gt;. Thay vào đó, &lt;code&gt;L3&lt;/code&gt; giờ đóng vai trò chỉ huy, nhằm mục đích thống nhất về giá trị &lt;code&gt;v3&lt;/code&gt; giữa các phó tướng còn lại. Đây là lời giải theo kiểu &lt;em&gt;đệ quy&lt;/em&gt;. Do lời giải này chỉ có ý nghĩa khi &lt;code&gt;C&lt;/code&gt; là gián điệp, nên số gián điệp trong hệ thống &amp;quot;con&amp;quot; này là &lt;code&gt;m-1&lt;/code&gt;. Quá trình đệ quy này được lặp lại &lt;code&gt;m&lt;/code&gt; lần đối với mỗi phó tướng, cho đến trường hợp &lt;code&gt;m=0&lt;/code&gt; (là trường hợp đơn giản đã bàn ở trên).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance của lời giải&lt;/strong&gt;: Độ phức tạp (complexity) xét về mặt số lượng thông tin cần gửi đi của lời giải này là &lt;code&gt;O(n^m)&lt;/code&gt;, do kết quả của quá trình đệ quy. Điều này có nghĩa là với lời giải trên có tính thực tế không cao trong các hệ thống lớn gồm nhiều process (do số thông tin cần gửi ở đây tăng theo cấp số nhân). Đã có nhiều nghiên cứu cải tiến lời giải nhằm cải thiện hiệu năng của bài toán này, như pBFT, Speculative Byzantine Fault Tolerance...&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ung-dung-thuc-te-cua-thuat-toan-bft"&gt;Ứng dụng thực tế của thuật toán BFT&lt;/h3&gt;
&lt;p&gt;Thuật toán BFT (và các biến thể) nhằm giải quyết vấn đề consensus cho các hệ thống mà có thể sẽ gặp phải kiểu sự cố Byzantine (mời xem lại định nghĩa về kiểu sự cố này ở &lt;a href="/posts/distributed-system-overview"&gt;phần 1&lt;/a&gt;). Ví dụ về các hệ thống này bao gồm Blockchain, các hệ thống hàng không vũ trụ của NASA hay SpaceX...&lt;/p&gt;
&lt;p&gt;Tuy nhiên, trong hầu hết các ứng dụng Internet thực tế, kiểu sự cố Byzantine hiếm khi xảy ra (so với các kiểu sự cố khác như fail-stop, fail-recover). Lý do là vì: 1) các hệ thống DS như Database, Message Queue ... thường được triển khai ở trong một môi trường an toàn (thường là trong các data center có sự kiểm soát tốt về mặt kết nối mạng thông qua tường lửa...), tách biệt với các cá thể có ý định tấn công từ bên ngoài; 2) kết nối của hệ thống với môi trường Internet bên ngoài thường được mã hóa dựa trên những giao thức bảo mật như SSL. VD như, khi bạn đọc blog này với đường link bao gồm &amp;quot;&lt;em&gt;https&lt;/em&gt;&amp;quot;, bạn được đảm bảo rằng nội dung của blog thực sự đến từ người viết chứ không bị đánh tráo; 3) các giao thức được chuẩn hóa như TCP/IP, HTTP... cũng giúp bảo vệ cho hệ thống khỏi những lỗi Byzantine được gây ra do trục trặc từ phần cứng.&lt;/p&gt;
&lt;p&gt;Nói thế để thấy rằng, tuy thuật toán BFT tương đối phức tạp và có hiệu năng thấp, chúng ta cũng không cần phải lo đến việc áp dụng thuật toán này cho dự án của mình. Trong các phần sau, chúng ta sẽ bàn thêm về các thuật toán Consensus cho các hệ thống được coi như &amp;quot;miễn dịch&amp;quot; với kiểu sự cố Byzantine.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Ví dụ về Distributed Systems được phát triển và sử dụng trong thực tế bao gồm hệ thống Database (SQL, NoSQL), hệ thống Cache (Redis, Memcached), hệ thống Message-Queue hay Publish/Subscribe (Kafka, RabbitMQ ...). Những hệ thống này tuy có những tính năng khác nhau, tuy nhiên phần cốt lõi của chúng đều phải giải quyết một vấn đề cơ bản, đó là bài toán &lt;strong&gt;đồng thuận&lt;/strong&gt;.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://dhhoang.github.io/posts/distributed-system-overview</id>
		<title>[P1] Tổng quan về Distributed Systems</title>
		<link href="http://dhhoang.github.io/posts/distributed-system-overview" />
		<updated>2020-08-26T00:00:00Z</updated>
		<content>&lt;p&gt;Đây là bài viết mở đầu trong series bài viết về đề tài Distributed Systems. Dự định của mình là sẽ thực hiện loạt bài viết tương đối chuyên sâu về các chủ đề: Distributed Systems, Microservices, Transactions, Event sourcing, CQRS, Domain Driven Design, và nếu có thời gian, có thể sẽ đề cập đến cả Blockchain nữa.&lt;/p&gt;
&lt;h2 id="so-luoc"&gt;Sơ lược&lt;/h2&gt;
&lt;p&gt;Một &amp;quot;hệ thống phân tán&amp;quot; (Distributed Systems-từ giờ xin được phép sử dụng từ gốc tiếng Anh) được định nghĩa là một tập hợp các tiến trình điện toán (process) độc lập, được kết nối với nhau bởi một hệ thống mạng (network) để các process này có thể truyền nhận thông tin  ('process' ở đây được định nghĩa là một đơn vị điện toán được vận hành với một không gian bộ nhớ-memory space-riêng biệt, không trùng lặp với các process khác). Các process này phối hợp hoạt động với nhau như một thực thể duy nhất đối với người dùng bên ngoài  nhằm thực thi một nhiệm vụ nào đó. Dựa theo định nghĩa này thì nhiều process trên cùng 1 máy tính cũng có thể được coi như Distributed Systems (DS). Dĩ nhiên trên thực tế, người ta quan tâm đến việc vận hành nhiều máy tính cùng với nhau, cho nên chúng ta có thể ngầm hiểu là các process này chạy trên các máy tính riêng biệt.&lt;/p&gt;
&lt;p&gt;Vậy &lt;strong&gt;tại sao chúng ta cần tìm hiểu về DS&lt;/strong&gt;? DS xuất hiện rất phổ biến trên thực tế. Hầu hết các ứng dụng ngày nay, đặc biệt là các ứng dụng Internet, đều được triển khai dưới dạng DS. Việc triển khai phần mềm, đặc biệt là những hệ thống lớn, trên nhiều đơn vị máy tính (thay vì chỉ dùng một máy tính duy nhất) có rất nhiều lợi ích, VD như:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cung cấp nhiều tài nguyên hơn khi hệ thống cần xử lý lượng công việc lớn hơn.&lt;/li&gt;
&lt;li&gt;Chỉ dùng một đơn vị máy tính đồng nghĩa với rủi ro phần mềm bị sập (crash) nếu máy đó gặp sự cố. Dùng nhiều đơn vị máy sẽ cho phép bạn tiếp tục vận hành phần mềm kể cả khi sự cố xảy ra.&lt;/li&gt;
&lt;li&gt;Khi hệ thống của bạn trở nên phức tạp và cần sự kết hợp từ nhiều thành phần khác nhau, sử dụng DS cho phép bạn chia nhỏ một hệ thống to thành nhiều đơn vị nhỏ. Mỗi đơn vị có thể hoạt động độc lập, thậm chí có thể được phát triển bởi các team khác nhau với các chuyên môn khác nhau.&lt;/li&gt;
&lt;li&gt;Người dùng hệ thống có thể có sự phân tán về mặt địa lý trên toàn cầu. Để đảm bảo chất lượng dịch vụ và hạn chế độ trễ thì hệ thống máy cũng cần được phân tán để có thể có mặt ở gần người dùng nhất có thể.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="xay-dung-distributed-systems-co-gi-kho"&gt;Xây dựng Distributed Systems có gì khó ?&lt;/h2&gt;
&lt;p&gt;Tuy mang đến nhiều lợi ích như kể trên, DS cũng làm cho việc phát triển và vận hành trở nên phức tạp hơn, do đặc tính phân tán của nó. Để xây dựng một hệ thống DS hoạt động tốt, chúng ta sẽ phải giải quyết rất nhiều bài toán, tùy thuộc vào đặc thù của hệ thống đó. Có thể kể ra một vài bài toán thường gặp như:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bài toán về xử lý sự cố (failure): Với một hệ thống máy tính lớn thì việc máy móc gặp trục trặc hay sự cố là điều thường xuyên xảy ra. Xử lý sự cố để không làm ảnh hưởng đến hệ thống (hoặc ít ra là giảm thiểu ảnh hưởng) là vấn đề cốt lõi nhất trong việc xây dựng DS.&lt;/li&gt;
&lt;li&gt;Vấn đề đồng thuận về dữ liệu (consensus): Xây dựng DS đồng nghĩa với việc dữ liệu của hệ thống cũng sẽ bị phân tán. Điều này gây ra trở ngại nếu chúng ta muốn hệ thống làm việc như một thực thể thống nhất, vì các máy đôi khi sẽ bất đồng với nhau về mặt dữ liệu.&lt;/li&gt;
&lt;li&gt;Vấn đề bất đồng bộ về mặt thời gian: Mỗi máy tính trong hệ thống có một đồng hồ riêng biệt, và không nhất quán với nhau về mặt thời gian. Đồng hồ máy tính cũng thường xuyên xảy ra tình trạng chạy lệch pha (clock drift). Sự bất đồng bộ này có thể dẫn đến sai lệch về mặt logic của hệ thống. VD như, nếu một ứng dụng nhắn tin không thống nhất được về mặt thời gian, thứ tự tin nhắn của người dùng có thể sẽ sai lệch giữa người dùng này với người dùng kia.&lt;/li&gt;
&lt;li&gt;Các vấn đề khác như vấn đề bảo mật, vấn đề giao tiếp, vấn đề lưu trữ và sao lưu dữ liệu...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Có một điều thú vị trong lĩnh vực DS, đó là các bài toán kể trên hầu như không có lời giải lý thuyết nào đáp ứng được yêu cầu một cách hoàn hảo trên (sẽ được bàn luận sâu hơn trong các bài sau). Tuy nhiên điều này không có nghĩa là chúng không có những cách giải quyết phù hợp trong thực tế, tùy vào từng hệ thống cụ thể. Hầu hết các tiến triển gần đây trong lĩnh vực DS đều nhắm đến việc xây dựng những lời giải thực tế cho những hệ thống với mục đích cụ thể.&lt;/p&gt;
&lt;h2 id="cac-kieu-su-co-trong-distributed-systems"&gt;Các kiểu sự cố trong Distributed Systems&lt;/h2&gt;
&lt;p&gt;Như đã nói ở trên, sự cố trong vận hành hệ thống xảy ra một cách thường xuyên. Sau đây là đoạn lược dịch lời &lt;strong&gt;Jeff Dean&lt;/strong&gt;, một kĩ sư trưởng tại Google:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Trong trung tâm dữ liệu (data center) của Google, mỗi năm một cluster sẽ xảy ra khoảng 1000 sự cố máy, hàng ngàn sự cố ổ cứng, trung bình một sự cố về nguồn điện khiến cho khoảng 500-1000 server bị sập trong vòng khoảng 6 tiếng đồng hồ. Ngoài ra, trung bình 5 rack sẽ gặp trục trặc, làm mất đi một nửa số packet được truyền tải, làm ảnh hưởng đến khoảng 5% số server tại bất kì thời điểm nào.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Vì sự cố xảy ra thường xuyên như vậy, nên các hệ thống DS cần có cơ chế tự động xử lý khi gặp phải tình huống xấu, vì không phải lúc nào con người cũng có thể can thiệp một cách kịp thời. Để giải quyết được vấn đề này thì chúng ta cần hiểu được hệ thống có thể gặp phải những kiểu sự cố nào.&lt;/p&gt;
&lt;h3 id="su-co-ve-may-node-failure"&gt;Sự cố về máy (node failure)&lt;/h3&gt;
&lt;p&gt;Mỗi máy tính vật lý, do nhiều nguyên nhân khác nhau, có thể gặp sự có trong khi hoạt động. Người ta chia các sự cố này thành một vài loại chính:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fail-stop&lt;/strong&gt;: Đây là kiểu sự cố khiến cho process trên máy dừng hoạt động (dừng tính toán cũng như truyền nhận tin). Nguyên nhân sự cố này có thể do máy bị sập (lỗi phần mềm, lỗi hệ điều hành ...), lỗi phần cứng, hay những nguyên nhân bên ngoài (vd như mất điện chẳng hạn). Đây là kiểu sự cố thường gặp nhất, nên khi người ta nói đến 'failure' mà không nhắc gì thêm thì thường được ngầm hiểu là kiểu sự cố này. Hầu hết các thuật toán được phát triển trong DS đều nhằm đối phó với kiểu sự cố này.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fail-recover&lt;/strong&gt;: Process có thể ngừng hoạt động trong một thời gian nhất định, nhưng sau đó phục hồi hoạt động trở lại. Nguyên nhân của kiểu sự cố này có thể là do máy tự động reboot do một nguyên nhân nào đó. Thường khi nhắc đến kiểu sự cố này, người ta mặc đình rằng máy có khả năng lưu trữ thông tin vào ổ đĩa cứng và phục hồi thông tin này sau khi sự cố xảy ra.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Byzantine failure&lt;/strong&gt;: Sự cố mà máy tính không hoạt động theo đúng yêu cầu đề ra. VD như, máy có thể gửi tin tùy ý, hay thay đổi trạng thái tùy ý, không giống những gì được lập trình. Đây là kiểu sự cố &lt;em&gt;khó chịu&lt;/em&gt; nhất, có thể xảy ra khi hệ thống gặp trục trặc không rõ nguyên nhân (vd như RAM có thể bị hỏng khiến xảy ra tình trạng bit-flip), hay do hệ thống bị kể xấu tấn công.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="su-co-ve-network"&gt;Sự cố về network&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://github.com/dhhoang/dhhoang.github.io/raw/gh-pages/network_partition.JPG" class="img-fluid" alt="Network partition" title="Logo Title Text 1" /&gt;
Mạng máy tính cũng là một sản phẩm vật lý và vì vậy cũng có thể xảy ra sự cố. Một kiểu sự cố thường gặp là sự cố &amp;quot;chia cắt mạng&amp;quot; (&lt;strong&gt;network partitioning&lt;/strong&gt;), được mô phỏng bởi hình trên. Sự cố này xảy ra đường truyền của một hoặc nhiều server bị chia cắt khỏi phần còn lại của hệ thống, khiến hệ thống bị chia cắt thành nhiều phần không thể giao tiếp với nhau.
Trên thực tế, trong các data center, một cluster máy chủ thường được kết nối với nhau bởi một hoặc nhiều cục switch. Sự cố của cổng switch hoặc dây dẫn có thể dẫn đến việc một hay nhiều server bị ngắt kết nối, dẫn đến tình trạng partitioning kể trên.&lt;/p&gt;
&lt;p&gt;Ngoài ra, một vài sự cố mạng khác có thể kể ra như việc latency bị tăng cao (do congestion control), hay network adapter của các server gặp trục trặc ...&lt;/p&gt;
&lt;h3 id="thiet-ke-distributed-systems-e-san-sang-xu-ly-cac-su-co"&gt;Thiết kế Distributed Systems để sẵn sàng xử lý các sự cố&lt;/h3&gt;
&lt;p&gt;Do sự cố xảy ra thường xuyên và đa dạng như đã kể trên, hệ thống DS cần được thiết kế để có cơ chế tự động xử lý sự cố, đảm bảo việc vận hành không bị gián đoạn hay sai sót. Sau đây mình sẽ trích đoạn một bài báo khoa học có tên là &lt;a href="https://www.usenix.org/legacy/event/lisa07/tech/full_papers/hamilton/hamilton.pdf"&gt;&amp;quot;Về việc thiết kế và triển khai các dịch vụ Internet&amp;quot;&lt;/a&gt;, của tác giả &lt;strong&gt;James Hamilton&lt;/strong&gt; (khi đó làm việc tại Microsoft, hiện tại là kĩ sư trưởng tại Amazon Web Service):&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Thiết kế hệ thống để xử lý sự cố&lt;/strong&gt; là khái niệm cốt lõi khi phát triển các dịch vụ quy mô lớn, bao gồm nhiều thành phần nhỏ. Những thành phần này sẽ thường xuyên gặp sự cố, và đôi khi sự cố này có thể dẫn đến sự cố kia. Khi hệ thống được triển khai trên khoảng 10,000 server và 50,000 đơn vị đĩa cứng, sự cố sẽ xảy ra nhiều lần trong một ngày. Nếu sự cố nào cũng cần đến con người can thiệp, hệ thống sẽ không thể vận hành một cách trơn tru và tiết kiệm được chi phí. Do đó, hệ thống cần có cơ chế đối phó với sự cố mà không cần con người can thiệp. Cơ chế này cần được kiểm tra một cách thường xuyên. Một cách đơn giản để kiểm tra là cố tình gây ra sự cố thường xuyên trong quá trình vận hành hệ thống. Điều này thoạt nghe có vẻ vô lý, tuy nhiên, nếu cơ chế sự cố không thường xuyên được sử dụng thì chúng sẽ không hoạt động khi cần thiết.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nói thế để thấy rằng, khi phát triển một hệ thống DS, vấn đề sự cố luôn phải được coi như một phần của bài toán, và chúng ta tuyệt đối không được mặc định rằng sự cố không bao giờ xảy ra.&lt;/p&gt;
&lt;h2 id="mot-vai-tai-lieu-uong-link-e-tim-hieu-sau-them-ve-distributed-systems"&gt;Một vài tài liệu &amp;amp; đường link để tìm hiểu sâu thêm về Distributed Systems&lt;/h2&gt;
&lt;p&gt;Distributed Systems là một lĩnh vực tương đối sâu rộng và trong một vài bài viết thì khó mà chúng ta có thể đề cập chi tiết hết về các chủ đề được. Sau đây mình chia sẻ một vài nguồn thông tin để các bạn có thể tìm hiểu sâu hơn về lĩnh vực này:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Khóa &lt;strong&gt;Distributed Systems&lt;/strong&gt; của MIT được ghi lại trên YouTube: &lt;a href="https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB"&gt;https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kênh &lt;strong&gt;Distributed Systems Course&lt;/strong&gt; trên Youtube: &lt;a href="https://www.youtube.com/user/cbcolohan"&gt;https://www.youtube.com/user/cbcolohan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Khóa học &lt;strong&gt;Reliable Distributed Algorithms&lt;/strong&gt; (miễn phí): &lt;a href="https://courses.edx.org/courses/course-v1:KTHx+ID2203.1x+3T_2017/course/"&gt;phần 1&lt;/a&gt; và &lt;a href="https://courses.edx.org/courses/course-v1:KTHx+ID2203.2x+2016T4/course/"&gt;phần 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trong phần sau, mình sẽ bàn về bài toán đồng thuận (Consensus), một bài toán cơ bản nhất trong Distributed Systems.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Đây là bài viết mở đầu trong series bài viết về đề tài Distributed Systems. Dự định của mình là sẽ thực hiện loạt bài viết tương đối chuyên sâu về các chủ đề: Distributed Systems, Microservices, Transactions, Event sourcing, CQRS, Domain Driven Design, và nếu có thời gian, có thể sẽ đề cập đến cả Blockchain nữa.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://dhhoang.github.io/posts/design-tinyurl</id>
		<title>Designing TinyURL: it's more complicated than you think</title>
		<link href="http://dhhoang.github.io/posts/design-tinyurl" />
		<updated>2020-08-10T00:00:00Z</updated>
		<content>&lt;p&gt;How would you design a service like &lt;a href="https://tinyurl.com/"&gt;TinyURL&lt;/a&gt; or &lt;a href="https://bitly.com/"&gt;Bitly&lt;/a&gt; ?&lt;/p&gt;
&lt;p&gt;Recently I came across a Youtube video called: &lt;a href="https://www.youtube.com/watch?v=fMZMm_0ZhK4"&gt;&lt;strong&gt;System Design : Design a service like TinyUrl&lt;/strong&gt;&lt;/a&gt;, from the channel &lt;strong&gt;Tushar Roy - Coding Made Simple&lt;/strong&gt;. This video discusses a common developer interview question, namely, how do you design a service like TinyURL, which allows users to turn long URLs into short ones that are just several characters long (&lt;a href="https://www.youtube.com/watch?v=fMZMm_0ZhK4"&gt;https://www.youtube.com/watch?v=fMZMm_0ZhK4&lt;/a&gt;).
Basically, a TinyURL-like service would have 2 main APIs: &lt;code&gt;createShort(longUrl)&lt;/code&gt; and &lt;code&gt;getLong(shortUrl)&lt;/code&gt;. The second one is easy, you simply need to do a lookup and return the long URL (or 404 if none exists). The main problem is the &lt;code&gt;createShort()&lt;/code&gt; API: How do you generate a short sequence of characters that is unique among URLs (note that &lt;em&gt;uniqueness&lt;/em&gt; is an important property, we don't want different URLs to have the same shortcut).&lt;/p&gt;
&lt;p&gt;Tushar's proposed solutions are quite good and I think most interviewers would be satisfied with them (please watch the video before continuing to read this post). That being said, they are sort of unsatisfying. To summarize, the most sophisticated solution proposed in the video is to partition all possible short sequences into ranges, and use a set of servers to return a monotonically increasing sequence, which falls within a range. Each server would be in assigned only one particular range to work with, and &lt;a href="https://zookeeper.apache.org/"&gt;Apache Zookeeper&lt;/a&gt; is used to coordinate the sequence range assignments.
If the each server has a unique range, then they are guaranteed to generate unique sequences.
&lt;img src="https://dev-to-uploads.s3.amazonaws.com/i/66rjp2ympf34jjdy5xkj.png" class="img-fluid" alt="Design Tiny-URL like service using Zookeeper" /&gt;
The reason I think this answer is unsatisfying is because, while it works, it simply shifts the responsibility of generating the &amp;quot;unique&amp;quot; part of the sequence, which, is the hardest part of the problem, to Zookeeper. Instead of answering the question &amp;quot;how to generate a unique sequence?&amp;quot; (or sequence range, in this case), this solution simply says &amp;quot;I'll just ask Zookeeper to give me one&amp;quot;. But how does Zookeeper do that?&lt;/p&gt;
&lt;p&gt;First of all, why is it so hard to generate a unique sequence? Afterall, I can use a single computer to keep increasing a counter, and that would be unique, right? In fact, that solution is mentioned by Tushar in the video, but later rejected, because the counter-generating server might fail (either the machine itself crashes, or the network might go down etc.), and Zookeeper, somehow, magically provides &amp;quot;high availability&amp;quot; (i.e. it is resilient to failures).&lt;/p&gt;
&lt;p&gt;And that's the gist of the problem. If I had the guarantee that my servers &lt;em&gt;never&lt;/em&gt; fails, then I wouldn't need Zookeeper. I probably wouldn't need multiple servers either, one beefy machine might be enough to do the job. Unfortunately, in practice machines do fail, and in fact, they fail &lt;em&gt;all the time&lt;/em&gt;. That is why when we design systems, we &lt;a href="https://www.usenix.org/legacy/event/lisa07/tech/full_papers/hamilton/hamilton_html/index.html"&gt;design for failure&lt;/a&gt;. In this case, when one servers in the Zookeeper cluster fails, somehow the system needs to make sure that the others don't return a duplicate range. The only way to do that is to make all servers agree on which ranges have been given, and which have not.&lt;/p&gt;
&lt;p&gt;So let's try to simplify &amp;amp; generalize the problem: given a set of servers, how do we ensure that all servers agree on a value, even if the servers might fail randomly (the &lt;em&gt;value&lt;/em&gt; in this case would be the range assignment). This is know as the &lt;em&gt;distributed consensus problem&lt;/em&gt;, which actually is one of the hardest problems in Distributed systems. In fact, it has been mathematically proven that, in an asynchronous system (meaning a system where we don't know how long it takes for messages to travel between servers), there is &lt;strong&gt;NO&lt;/strong&gt; way to guarantee distributed consensus. This is known as the FLP Impossibility.&lt;/p&gt;
&lt;p&gt;Fortunately, in most of the systems in practice, we can workaround this issue by modelling them as &amp;quot;partially synchronous systems&amp;quot;, that is, we can apply a boundary on how long it takes to send messages between servers. And in this model, consensus is possible. There are several algorithms that can be used to get consensus, like &lt;a href="https://www.cs.rutgers.edu/%7Epxk/417/notes/paxos.html"&gt;Paxos&lt;/a&gt; or &lt;a href="https://raft.github.io/"&gt;Raft&lt;/a&gt;. Zookeeper itself uses a consensus protocol called &lt;a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab1.0"&gt;Zab&lt;/a&gt; (which stands for Zookeeper atomic broadcast).&lt;/p&gt;
&lt;p&gt;I won't get into details on how these algorithms work. Afterall, they are quite complicated and sometimes difficult to understand. However if you ever need to work with those directly, an important thing to pay attention to is that they are not perfect. Raft and Paxos, for example, only works if the number of failed nodes is less than half the total number of nodes in the system. Failure also take different forms, and while Paxos and Raft works well with Fail-stop and Fail-safe types of failure, Byzantine-type failures are a lot harder to deal with.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;How would you design a service like &lt;a href="https://tinyurl.com/"&gt;TinyURL&lt;/a&gt; or &lt;a href="https://bitly.com/"&gt;Bitly&lt;/a&gt; ?&lt;/p&gt;</summary>
	</entry>
</feed>